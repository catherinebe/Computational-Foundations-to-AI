# -*- coding: utf-8 -*-
"""AI Assignment2_Vectorized

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_6AQZuQRpA5rLQKT7AATKTTxuhouhvDX

Catherine Berrouet, Z23353674

Computational Foundations of AI, Fall 2020
 
Assignment 2

# Task

We will perform a penalized (regularized) least squares fit of a linear model using elastic net, with the model parameters obtained by coordinate descent. Elastic net will permit you to provide simultaneous parameter shrinkage (tuning parameter ðœ† â‰¥ 0) and feature selection (tuning parameter ð›¼ âˆˆ [0,1]). The two tuning parameters ðœ† and ð›¼ will be chosen using five-fold cross validation, and the best-fit model parameters will be inferred on the training dataset conditional on an optimal pair of tuning parameters.

Import Data & Libraries
"""

####################################
# Import Python libraries for data
####################################
import pandas as pd
import numpy as np
import time
# Libraries for plotting
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

####################################
# Import data from csv file uploaded onto google drive
####################################
from google.colab import drive
drive.mount('/content/drive')
# Import data from csv file uploaded onto google drive
data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Computational AI/Credit_N400_p9.csv')

# Reformatting categorical data into numerical binary values
datacopy = data # We use a copy and keep original import
clean = datacopy.replace({'Male': 0, 'Female':1})
clean = clean.replace({'No': 0, 'Yes': 1})
# clean

"""# Data"""

clean

"""## Functions

Variables Initialized:
X
Y
"""

# ####################################
# # Vectorized Algorithm
# ####################################

####################################
# Step 1 Variables Initialized: Data Design Matrix X and Y centered response data
####################################
training_data = clean.iloc[:, :-1]                                # we only take the columns with 9 features we want to analyze
X = (training_data - training_data.mean())/training_data.std()    # X is standardized
Y_data = pd.DataFrame(clean.iloc[:, -1])                          # dependent variables; the true values y_i (i.e Credit Balance column)
Y_centered = Y_data - Y_data.mean(axis=0)                         # Y is centered
Y = pd.DataFrame(Y_centered)
# pd.concat([X,Y], sort = False)                                  # concatenate two dataframes
N = X.shape[0]                                                    # 400 rows
p = X.shape[1]                                                    # 9 columns

####################################
# Step 2 Variables Initialized: Fix tuning parameter vectors: lambda and alpha
####################################
lambda_tuning = [10**(-2), 10**(-1), 10**(0), 10, 10**(2), 10**(3), 10**(4), 10**(5), 10**(6)]
alpha_tuning = [0,1/5, 2/5, 3/5, 4/5, 1]
max_iter = 1000                                                   # Maximum Iterations for convergence

####################################
# Step 3: Precompute b_k
####################################
def precompute_bk():
    ''' function returns precomputed b_k vector of length 9 '''
    bk = np.asarray(X.pow(2,axis = 1).sum(axis=0))
    return bk

####################################
# Step 4: randomized initialization beta vector from uniform distribution [-1,1]
####################################
def beta_vector():
    ''' function initializes a beta vector from uniform random distribution '''
    # Call on the function assigning to a variable as follows: test_beta = beta_vector()
    beta = np.array(np.random.uniform(low = -1.0, high = 1.0, size=p))
#    beta = beta.T  # Note: beta is a 1x9 vector, (ie dim(beta) = 1xp), beta.shape[1] = 9
    return beta

####################################
# Step 5: for each k in [1,p], compute a_k and set B_k
####################################
def compute_a_k(B,X,Y):
  '''
  This function computes the a_k term
  
  :param X: standardized X training data
  :param Y: N-dimensional centered Y true values data
  :param B: beta randomly initialized parameter vector from uniform distribution [-1,1]
  
  :return ak: a_k computation for the cost function of the elastic net fit
  '''
  B = B.T
  for k in range(p):
      B = pd.DataFrame(B)
      Bk = B.iloc[k].values
      xk = pd.DataFrame(X.iloc[:,k])
      XB = X.dot(B.values)
      xkt = xk.T
      ak =  xkt.dot(Y.values - XB.values + (xk.values * Bk))
      return ak.squeeze() # pandas.core.frame.DataFrame

def sign(x):
  ''' 
  function determines output if x is positive or negative
  :param x: pandas.core.frame.DataFrame syntax, output of compute_a_k function
  
  :return integer: 1 or -1 depending on conditions defined
  '''
  # print('debugging sign function:')
  # print(x)

  if x >= 0:
    return 1
    # return np.sign(x)
  elif x < 0:
    return -1
  elif isinstance(x, list) or isinstance(x, pd.Series) and len(x)>1:
    return sign_CV(x)

def sign_CV(x):
  ''' function computes sign for an array and pandas or series syntax x '''
  # print('reached this sign_CV function')
  # print('input:')
  # # print(x)
  # print('values syntax')
  # print(x.values[0])
  # print(len(x))

  total = []
  for element in range(len(x)):
    # print('looping here...')
    r = (sign(x.iloc[element])) # numpy.float64
    # print('r:', r)
    return sign(r)
  else:
    return sign(x.values[0])


  # if len(x)>1:
  #   total = []
  #   for element in range(len(x)):
  #     # total.append(sign(x.iloc[element])) # numpy.float64
  #     # return sign_CV(total)
  #     r = (sign(x.iloc[element])) # numpy.float64
  #     print('r:', r)
  #     return sign(r)
  # else:
  #   return sign(x.values[0])

# Single B_k computation for Gradient Coordinate Descent
def compute_B_k(ak, lambda_value, alpha_value, bk):
    ''' 
    Function sets B_k for Bhat vector of optimal values for alpha and lambda using a_k 
    
    :param ak: array from pandas.core.frame.DataFrame
    :param lambda_value: single numerical value
    :param alpha_value: integer
    :param bk: the k-th value of precomputed b_k
    
    :return B_k: computed integer
    '''

    # print('debugging ak for sign function')
    # # ak = ak.squeeze()
    # ak = ak.values
    # print(ak)
    # print('type(ak)= ', type(ak))
    # print('len = ', len(ak))

    # Note: added if/else conditionals for folds when ak is multiple scalars
    if isinstance(ak, pd.Series) and len(ak)>1:
      ak = sign_CV(ak)
      num_term1 = sign(ak)
      num_term2 = abs(ak) - ((lambda_value)*(1-alpha_value))/2
      denominator = bk + (lambda_value)*(alpha_value)
      plus = max(0, num_term2)    
      B_k = (num_term1 * plus) / denominator
      return B_k
    
    else:
      num_term1 = sign(ak)
      num_term2 = abs(ak) - ((lambda_value)*(1-alpha_value))/2
      denominator = bk + (lambda_value)*(alpha_value)
      plus = max(0, num_term2)    
      B_k = (num_term1 * plus) / denominator
      return B_k # numpy.ndarray

# compute_a_k(B,X,Y) # needs to be a scalar

####################################
# Coordinate Gradiant Descent Functions
####################################

def coordinate_descent_inner(X, Y, beta, alpha_value, lambda_value, bk):
  '''
  Function computes the coordinate descent and returns the updated parameter vector Bhat
  
  :param X: standardized Nxp design matrix of training data
  :param Y: centered N-dimensional response vector
  :param alpha_tuning: single alpha value out of the tuning vector list of 6 values
    (i.e. alpha_tuning = [0,1/5, 2/5, 3/5, 4/5, 1])
  :param lambda_tuning: single lambda value from the tuning vector, list of 9 values
    (lambda_tuning = [10**(-2), 10**(-1), 10**(0), 10, 10**(2), 10**(3), 10**(4), 10**(5), 10**(6)])
  :param max_iter: fixed iterations at 1000 (we assume max iters for convergence)
  :param b_k: precomputed b sub k vector across features

  :return beta: the beta parameter vector with updated values
  '''  
  p = X.shape[1]         # 9 columns
  beta = beta.T  
  all_aks = []
  # compute updated beta[k] value using a_k and b_k
  for k in range(p):                  # (9)
      ak = compute_a_k(beta,X,Y)
      all_aks.append(ak.squeeze())
      beta[k] = compute_B_k(ak, lambda_value, alpha_value, bk[k])
  # print('ak values computed:', np.asarray(set(all_aks)))
  return beta

def coordinate_descent(dX, dy, beta, alpha, lamb, bk, max_iter = 1000): 
  '''
  :param dX: X (Nxp design matrix)
  :param dy: Y centered data
  :parm alpha: alpha_tuning parameter vector
  :param lamb: lambda_tuning parameter vector
  :param bk: precomputed betak vector, use precompute_bk()
  :param max_iter: integer, total iterations allotted for convergence
  '''
  beta_old = beta
  beta_new = coordinate_descent_inner(dX, dy, beta_old, alpha, lamb, bk)
  for i in range(max_iter-1):
    if np.array_equal(beta_old, beta_new):
      return beta_new
    else:
      beta_old = beta_new
      beta_new = coordinate_descent_inner(dX, dy, beta_old, alpha, lamb, bk)
  return beta_new

r = coordinate_descent(X, Y, beta_vector(), alpha_tuning[0], lambda_tuning[0], precompute_bk(), max_iter)
output1_iteration = pd.DataFrame(r).T
output1_iteration.columns = ['bhat0', 'bhat1', 'bhat2', 'bhat3', 'bhat4', 'bhat5', 'bhat6', 'bhat7', 'bhat8']
print('After 1 iteration check output:')
output1_iteration

####################################
# lambda function for graphing
####################################
def for_all_lambdas(dX, dy, beta, alpha, lamb, bk):
  # single alpha value
  # lamb is lambda tuning vector
  ''' function for graphs and plotting alphas'''
  param = pd.Series([])
  for i in range(len(lamb)):
    var = coordinate_descent(dX, dy, beta, alpha, lamb[i], bk)
    var = pd.Series(var, index = list(dX.columns))
    if param.empty:
      param = var 
    else:
      param = pd.concat([param,var], axis = 1)
    beta = beta_vector()
  param.columns = lamb
  param = param.T 
  return param

alpha0 = for_all_lambdas(X, Y, beta_vector(), alpha_tuning[0], lambda_tuning, precompute_bk())
alpha02 = for_all_lambdas(X, Y, beta_vector(), alpha_tuning[1], lambda_tuning, precompute_bk())
alpha04 = for_all_lambdas(X, Y, beta_vector(), alpha_tuning[2], lambda_tuning, precompute_bk())
alpha06 = for_all_lambdas(X, Y, beta_vector(), alpha_tuning[3], lambda_tuning, precompute_bk())
alpha08 = for_all_lambdas(X, Y, beta_vector(), alpha_tuning[4], lambda_tuning, precompute_bk())
alpha1 = for_all_lambdas(X, Y, beta_vector(), alpha_tuning[5], lambda_tuning, precompute_bk())

"""## Outputs for alpha's

Data output for alpha = 0
"""

alpha0

"""Data Output for alpha = 1/5"""

alpha02

"""Data Output for alpha = 2/5"""

alpha04

"""Data output for alpha = 3/5"""

alpha06

"""Data Output for alpha = 4/5"""

alpha08

"""Data Output for alpha = 1"""

alpha1

"""# Deliverable 1

Illustrate the effect of the tuning parameter on the inferred elastic net regression coefficients by generating six plots (one for each ð›¼ value) of nine lines (one for each of the ð‘ = 9 features).
"""

## Plot for $\alpha = 0$
sns.lineplot(data=alpha0,dashes=False).set_title('Alpha = 0')
plt.xscale('log')
plt.xlabel("Lambda", fontsize=15)
plt.ylabel("Standardized Coefficients", fontsize=15)
plt.show()

## Plot for $\alpha = \frac{1}{5}$
sns.lineplot(data=alpha02,dashes=False).set_title('Alpha = 0.2')
plt.xscale('log')
plt.xlabel("Lambda", fontsize=15)
plt.ylabel("Standardized Coefficients", fontsize=15)
plt.show()

## Plot for $\alpha = \frac{2}{5}$
sns.lineplot(data=alpha04,dashes=False).set_title('Alpha = 0.4')
plt.xscale('log')
plt.xlabel("Lambda", fontsize=15)
plt.ylabel("Standardized Coefficients", fontsize=15)
plt.show()

## Plot for $\alpha = \frac{3}{5}$
sns.lineplot(data=alpha06,dashes=False).set_title('Alpha = 0.6')
plt.xscale('log')
plt.xlabel("Lambda", fontsize=15)
plt.ylabel("Standardized Coefficients", fontsize=15)
plt.show()

## Plot for $\alpha = \frac{4}{5}$
sns.lineplot(data=alpha08,dashes=False).set_title('Alpha = 0.8')
plt.xscale('log')
plt.xlabel("Lambda", fontsize=15)
plt.ylabel("Standardized Coefficients", fontsize=15)
plt.show()

## Plot for $\alpha = 1$
sns.lineplot(data=alpha1,dashes=False).set_title('Alpha = 1')
plt.xscale('log')
plt.xlabel("Lambda", fontsize=15)
plt.ylabel("Standardized Coefficients", fontsize=15)
plt.show()

"""# Cross Validation"""

####################################
# Cross Validation Codes
####################################
df_val = clean.sample(frac=1).reset_index(drop=True)
# Used by passing the list of random numbers and removing them from the training indexes
# and from a list that keeps track of what already has been used

def rem_lst(org_ls,rem_ls,rand):
  for ele in rand:
    if ele in rem_ls:
      rem_ls.remove(ele)
    if ele in org_ls:
      org_ls.remove(ele)
  return org_ls, rem_ls, rand

####################################
# Standardize Validation
####################################
def val_stan(train, fold):
  ''' function that standardizes the validation sets '''
  a = train.subtract(train.mean())
  b = fold.subtract(train.mean())
  a1 = a.iloc[:,:-1]
  b1 = b.iloc[:,:-1]
  ty = a.iloc[:,-1]
  vy = b.iloc[:,-1]
  t = a1 / a1.std()
  v = b1 / a1.std()
  return t, ty, v, vy

####################################
# Folds and Training
####################################
import random
random.seed(9)
org_index = list(df_val.index)
r1,org1,k1 = rem_lst(org_index,org_index,random.sample(org_index,80))
# print(len(r1),len(org1),len(k1)) # For checking fold size

####################################
# Fold 1
####################################

fold1 = df_val.iloc[k1,:]
train1 = df_val.iloc[r1,:]
# y1 = df.iloc[k1,-1] # reponse vector
t1, ty1, f1, fy1 = val_stan(train1,fold1)
# new random beta vector for training CV
bk1 = t1.pow(2,axis = 1).sum(axis=0)

# alphas
a1_0 = for_all_lambdas(t1, ty1, beta_vector(), alpha_tuning[0], lambda_tuning, bk1) 
a1_1_5 = for_all_lambdas(t1, ty1, beta_vector(), alpha_tuning[1], lambda_tuning, bk1) 
a1_2_5 = for_all_lambdas(t1, ty1, beta_vector(), alpha_tuning[2], lambda_tuning, bk1) 
a1_3_5 = for_all_lambdas(t1, ty1, beta_vector(), alpha_tuning[3], lambda_tuning, bk1) 
a1_4_5 = for_all_lambdas(t1, ty1, beta_vector(), alpha_tuning[4], lambda_tuning, bk1) 
a1_1 = for_all_lambdas(t1, ty1, beta_vector(), alpha_tuning[5], lambda_tuning, bk1)

####################################
# Fold 2
####################################

random.seed(9)
org_index = list(df_val.index)
r2,org1,k2 = rem_lst(org_index,org1,random.sample(org1,80))
# print(len(r2),len(org1),len(k2))

fold2 = df_val.iloc[k2,:]
train2 = df_val.iloc[r2,:]
# y2 = df.iloc[k2,-1] 

t2, ty2, f2, fy2 = val_stan(train2, fold2)
bk2 = t2.pow(2,axis = 1).sum(axis=0)

#alphas
a2_0 = for_all_lambdas(t2, ty2, beta_vector(), alpha_tuning[0], lambda_tuning, bk2)
a2_1_5 = for_all_lambdas(t2, ty2, beta_vector(), alpha_tuning[1], lambda_tuning, bk2)
a2_2_5 = for_all_lambdas(t2, ty2, beta_vector(), alpha_tuning[2], lambda_tuning, bk2)
a2_3_5 = for_all_lambdas(t2, ty2, beta_vector(), alpha_tuning[3], lambda_tuning, bk2)
a2_4_5 = for_all_lambdas(t2, ty2, beta_vector(), alpha_tuning[4], lambda_tuning, bk2)
a2_1 = for_all_lambdas(t2, ty2, beta_vector(), alpha_tuning[5], lambda_tuning, bk2)

####################################
# Fold 3
####################################

random.seed(9)
org_index = list(df_val.index)
r3,org1,k3 = rem_lst(org_index,org1,random.sample(org1,80))
# print(len(r3),len(org1),len(k3))

fold3 = df_val.iloc[k3,:] 
train3 = df_val.iloc[r3,:] 
# y3 = df.iloc[k3,-1]

t3, ty3, f3, fy3 = val_stan(train3,fold3)
bk3 = t2.pow(2,axis = 1).sum(axis=0)

#alphas
a3_0 = for_all_lambdas(t3, ty3, beta_vector(), alpha_tuning[0], lambda_tuning, bk3)
a3_1_5 = for_all_lambdas(t3, ty3, beta_vector(), alpha_tuning[1], lambda_tuning, bk3)
a3_2_5 = for_all_lambdas(t3, ty3, beta_vector(), alpha_tuning[2], lambda_tuning, bk3)
a3_3_5 = for_all_lambdas(t3, ty3, beta_vector(), alpha_tuning[3], lambda_tuning, bk3)
a3_4_5 = for_all_lambdas(t3, ty3, beta_vector(), alpha_tuning[4], lambda_tuning, bk3)
a3_1 = for_all_lambdas(t3, ty3, beta_vector(), alpha_tuning[5], lambda_tuning, bk3)

####################################
# Fold 4
####################################

org_index = list(df_val.index)
r4,org1,k4 = rem_lst(org_index,org1,random.sample(org1,80))
print(len(r4),len(org1),len(k4))

fold4 = df_val.iloc[k4,:] 
train4 = df_val.iloc[r4,:] 
# y4 = df.iloc[k4,-1]

t4, ty4, f4, fy4 = val_stan(train4,fold4)
bk4 = t2.pow(2,axis = 1).sum(axis=0)

# alphas
a4_0 = for_all_lambdas(t4, ty4, beta_vector(), alpha_tuning[0], lambda_tuning, bk4)
a4_1_5 = for_all_lambdas(t4, ty4, beta_vector(), alpha_tuning[1], lambda_tuning, bk4)
a4_2_5 = for_all_lambdas(t4, ty4, beta_vector(), alpha_tuning[2], lambda_tuning, bk4)
a4_3_5 = for_all_lambdas(t4, ty4, beta_vector(), alpha_tuning[3], lambda_tuning, bk4)
a4_4_5 = for_all_lambdas(t4, ty4, beta_vector(), alpha_tuning[4], lambda_tuning, bk4)
a4_1 = for_all_lambdas(t4, ty4, beta_vector(), alpha_tuning[5], lambda_tuning, bk4)

####################################
# Fold 5
####################################

org_index = list(df_val.index)
r5,org1,k5 = rem_lst(org_index,org1,random.sample(org1,80))
print(len(r5),len(org1),len(k5))

fold5 = df_val.iloc[k5,:] 
train5 = df_val.iloc[r5,:] 
# y5 = df.iloc[k5,-1]

t5, ty5, f5, fy5 = val_stan(train5,fold5)
bk5 = t2.pow(2,axis = 1).sum(axis=0)

a5_0 = for_all_lambdas(t5, ty5, beta_vector(), alpha_tuning[0], lambda_tuning, bk5) 
a5_1_5 = for_all_lambdas(t5, ty5, beta_vector(), alpha_tuning[1], lambda_tuning, bk5) 
a5_2_5 = for_all_lambdas(t5, ty5, beta_vector(), alpha_tuning[2], lambda_tuning, bk5) 
a5_3_5 = for_all_lambdas(t5, ty5, beta_vector(), alpha_tuning[3], lambda_tuning, bk5) 
a5_4_5 = for_all_lambdas(t5, ty5, beta_vector(), alpha_tuning[4], lambda_tuning, bk5) 
a5_1 = for_all_lambdas(t5, ty5, beta_vector(), alpha_tuning[5], lambda_tuning, bk5)

####################################
# Cost Functions
####################################

def mse (X,y,beta,N):
  '''
  Calculates MSE
  :param X: standardized design matrix Nxp
  :param y: centered N-dimensional response vector
  :param beta: parameter vector
  :param N: integer representing row dimension of X; X.shape[0]
  '''
  mse_ls = list()
  for i in range(N):
    mse_ls.append((y.iloc[i]-(beta.dot(X.iloc[i])))**2)
  mse = sum(mse_ls)/N
  return mse

def mse_lamb(X,y,beta,elem):
  '''Calculates the mse for cross validation given a list of betas'''
  cv_ls = list()
  for i in range(elem):
    cv_ls.append(mse(X,y,beta.iloc[i],len(X)))
  return cv_ls

####################################
# Cost Calculations for CV Folds
####################################

fl1 = mse_lamb(f1,fy1,a1_0,len(a1_0)) 
fl2 = mse_lamb(f2,fy2,a2_0,len(a2_0)) 
fl3 = mse_lamb(f3,fy3,a3_0,len(a3_0)) 
fl4 = mse_lamb(f4,fy2,a4_0,len(a4_0)) 
fl5 = mse_lamb(f5,fy5,a5_0,len(a5_0))

fa0 = (mse_lamb(f1,fy1,a1_0,len(a1_0)),mse_lamb(f2,fy2,a2_0,len(a2_0)), 
      mse_lamb(f3,fy3,a3_0,len(a3_0)),mse_lamb(f3,fy3,a3_0,len(a3_0)), 
      mse_lamb(f4,fy2,a4_0,len(a4_0)),mse_lamb(f5,fy5,a5_0,len(a5_0)))

fa1 = (mse_lamb(f1,fy1,a1_1_5,len(a1_1_5)),mse_lamb(f2,fy2,a2_1_5,len(a2_1_5)),
       mse_lamb(f3,fy3,a3_1_5,len(a3_1_5)),mse_lamb(f3,fy3,a3_1_5,len(a3_1_5)), 
       mse_lamb(f4,fy2,a4_1_5,len(a4_1_5)),mse_lamb(f5,fy5,a5_1_5,len(a5_1_5)))

fa2 = (mse_lamb(f1,fy1,a1_2_5,len(a1_2_5)),mse_lamb(f2,fy2,a2_2_5,len(a2_2_5)), 
       mse_lamb(f3,fy3,a3_2_5,len(a3_2_5)),mse_lamb(f3,fy3,a3_2_5,len(a3_2_5)), 
       mse_lamb(f4,fy2,a4_2_5,len(a4_2_5)),mse_lamb(f5,fy5,a5_2_5,len(a5_2_5)))

fa3 = (mse_lamb(f1,fy1,a1_3_5,len(a1_3_5)),mse_lamb(f2,fy2,a2_3_5,len(a2_3_5)), 
       mse_lamb(f3,fy3,a3_3_5,len(a3_3_5)),mse_lamb(f3,fy3,a3_3_5,len(a3_3_5)), 
       mse_lamb(f4,fy2,a4_3_5,len(a4_3_5)),mse_lamb(f5,fy5,a5_3_5,len(a5_3_5)))

fa4 = (mse_lamb(f1,fy1,a1_4_5,len(a1_4_5)),mse_lamb(f2,fy2,a2_4_5,len(a2_4_5)), 
       mse_lamb(f3,fy3,a3_4_5,len(a3_4_5)),mse_lamb(f3,fy3,a3_4_5,len(a3_4_5)), 
       mse_lamb(f4,fy2,a4_4_5,len(a4_4_5)),mse_lamb(f5,fy5,a5_4_5,len(a5_4_5)))

fa5 = (mse_lamb(f1,fy1,a1_1,len(a1_1)),mse_lamb(f2,fy2,a2_1,len(a2_1)), 
       mse_lamb(f3,fy3,a3_1,len(a3_1)),mse_lamb(f3,fy3,a3_1,len(a3_1)), 
       mse_lamb(f4,fy2,a4_1,len(a4_1)),mse_lamb(f5,fy5,a5_1,len(a5_1)))

####################################
# Function for Output
####################################

def prep_cv_error(fold_list, index):
  cverr = pd.DataFrame()
  cverr['fold1'] = np.array(fold_list[0])
  cverr['fold2'] = np.array(fold_list[1])
  cverr['fold3'] = np.array(fold_list[2])
  cverr['fold4'] = np.array(fold_list[3])
  cverr['fold5'] = np.array(fold_list[4])
  cverr = cverr.rename(index={0:index[0],1:index[1],2:index[2],3:index[3],4:index[4],
                              5:index[5],6:index[6],7:index[7],8:index[8]})
  cverr_sum = cverr.sum(axis=1)/5
  return cverr, cverr_sum

####################################
# Output computation
####################################

cverr_df0,cverr0 = prep_cv_error(fa0,list(a1_0.index)) 
cverr_df1,cverr1 = prep_cv_error(fa1,list(a1_1_5.index)) 
cverr_df2,cverr2 = prep_cv_error(fa2,list(a1_2_5.index)) 
cverr_df3,cverr3 = prep_cv_error(fa3,list(a1_3_5.index)) 
cverr_df4,cverr4 = prep_cv_error(fa4,list(a1_4_5.index)) 
cverr_df5,cverr5 = prep_cv_error(fa5,list(a1_1.index))

####################################
# Convert to Pandas DataFrame
####################################

cverr = pd.DataFrame() 
cverr['0'] = np.array(cverr0) 
cverr['1/5'] = np.array(cverr1) 
cverr['2/5'] = np.array(cverr2) 
cverr['3/5'] = np.array(cverr3) 
cverr['4/5'] = np.array(cverr4) 
cverr['1'] = np.array(cverr5)
 # Labeling Columns
cverr = cverr.rename(index={0:cverr0.index[0],1:cverr0.index[1],2:cverr0. index[2],3:cverr0.index[3],
                              4:cverr0.index[4],5:cverr0.index[5],6:cverr0.index[6],7: cverr0.index[7],8:cverr0.index[8]})

cverr

"""# Deliverable 2

Illustrate the effect of the tuning parameters on the cross validation error by generating a plot of six lines (one for each ð›¼ value) with the ð‘¦-axis as CV error, and the ð‘¥- axis the corresponding log-scaled tuning parameter value log10(ðœ†) that generated the particular CV error. Label both axes in the plot. Without the log scaling of the tuning (5) parameter ðœ†, the plots will look distorted.
"""

sns.lineplot(data=cverr,dashes=False).set_title('CV Errors')
plt.xscale('log')
plt.xlabel("Lambda", fontsize=15)
plt.ylabel("CV Error", fontsize=15)
plt.show()

"""# Deliverable 3

Indicate the pair of values ðœ† and ð›¼ that generated the smallest CV error.
"""

for i in cverr.index:
  for j in cverr.columns:
    if cverr.loc[i,j] == cverr.min().min(): 
      # print(i,j)
      row, col = i,j
print("Alpha of Min: ", col )
print("Lambda of Min: ", row)
print("Value of Min: ", cverr.loc[row,col])

"""# Deliverable 4

Given the optimal ðœ† and ð›¼ pair, retrain your model on the entire dataset of ð‘ = 400 observations and provide the estimates of the ð‘ = 9 best-fit model parameters. How do these estimates compare to the estimates obtained from ridge regression (ð›¼ = 1 under optimal ðœ† for ð›¼=1) and lasso (ð›¼=0 under optimal ðœ† for ð›¼ =0) on the entire dataset of ð‘ = 400 observations?
"""

pd.DataFrame(alpha02.loc[row,:]).T

"""# Deliverable 5

Provide all your source code that you wrote from scratch to perform all analyses (aside from plotting scripts, which you do not need to turn in) in this assignment, along with instructions on how to compile and run your code.

## All Source Codes for running elastic net regression fit

```
# '''
# @author: catherineberrouet
# Assignment 2, Computational Foundations of AI
# Fall 2020
# ####################################
# # Vectorized Algorithm
# ####################################
# '''

# ####################################
# Import Python libraries for data
####################################
import pandas as pd
import numpy as np
import time
# Libraries for plotting
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

####################################
# Import data from csv file uploaded onto google drive
####################################
from google.colab import drive
drive.mount('/content/drive')
# Import data from csv file uploaded onto google drive
data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Computational AI/Credit_N400_p9.csv')

# Reformatting categorical data into numerical binary values
datacopy = data # We use a copy and keep original import
clean = datacopy.replace({'Male': 0, 'Female':1})
clean = clean.replace({'No': 0, 'Yes': 1})
# clean # print for viewing data

####################################
# Step 1 Variables Initialized: Data Design Matrix X and Y centered response data
####################################
training_data = clean.iloc[:, :-1]                                # we only take the columns with 9 features we want to analyze
X = (training_data - training_data.mean())/training_data.std()    # X is standardized
Y_data = pd.DataFrame(clean.iloc[:, -1])                          # dependent variables; the true values y_i (i.e Credit Balance column)
Y_centered = Y_data - Y_data.mean(axis=0)                         # Y is centered
Y = pd.DataFrame(Y_centered)
# pd.concat([X,Y], sort = False)                                  # concatenate two dataframes
N = X.shape[0]                                                    # 400 rows
p = X.shape[1]                                                    # 9 columns

####################################
# Step 2 Variables Initialized: Fix tuning parameter vectors: lambda and alpha
####################################
lambda_tuning = [10**(-2), 10**(-1), 10**(0), 10, 10**(2), 10**(3), 10**(4), 10**(5), 10**(6)]
alpha_tuning = [0,1/5, 2/5, 3/5, 4/5, 1]
max_iter = 1000                                                   # Maximum Iterations for convergence

####################################
# Step 3: Precompute b_k
####################################
def precompute_bk():
    ''' function returns precomputed b_k vector of length 9 '''
    bk = np.asarray(X.pow(2,axis = 1).sum(axis=0))
    return bk

####################################
# Step 4: randomized initialization beta vector from uniform distribution [-1,1]
####################################
def beta_vector():
    ''' function initializes a beta vector from uniform random distribution '''
    # Call on the function assigning to a variable as follows: test_beta = beta_vector()
    beta = np.array(np.random.uniform(low = -1.0, high = 1.0, size=p))
    return beta

####################################
# Step 5: for each k in [1,p], compute a_k and set B_k
####################################
def compute_a_k(B,X,Y):
  '''
  This function computes the a_k term
  
  :param X: standardized X training data
  :param Y: N-dimensional centered Y true values data
  :param B: beta randomly initialized parameter vector from uniform distribution [-1,1]
  
  :return ak: a_k computation for the cost function of the elastic net fit
  '''
  B = B.T
  for k in range(p):
      B = pd.DataFrame(B)
      Bk = B.iloc[k].values
      xk = pd.DataFrame(X.iloc[:,k])
      XB = X.dot(B.values)
      xkt = xk.T
      ak =  xkt.dot(Y.values - XB.values + (xk.values * Bk))
      return ak.squeeze() # pandas.core.frame.DataFrame

def sign(x):
  ''' 
  function determines output if x is positive or negative
  :param x: pandas.core.frame.DataFrame syntax, output of compute_a_k function
  
  :return integer: 1 or -1 depending on conditions defined
  '''
  if x >= 0:
    return 1
  elif x < 0:
    return -1
  elif isinstance(x, list) or isinstance(x, pd.Series) and len(x)>1:
    return sign_CV(x)

def sign_CV(x):
  ''' function computes sign for an array and pandas or series syntax x '''
  total = []
  for element in range(len(x)):
    r = (sign(x.iloc[element])) # numpy.float64
    return sign(r)
  else:
    return sign(x.values[0])

# Single B_k computation for Gradient Coordinate Descent
def compute_B_k(ak, lambda_value, alpha_value, bk):
    ''' 
    Function sets B_k for Bhat vector of optimal values for alpha and lambda using a_k 
    
    :param ak: array from pandas.core.frame.DataFrame
    :param lambda_value: single numerical value
    :param alpha_value: integer
    :param bk: the k-th value of precomputed b_k
    
    :return B_k: computed integer
    '''

    # Note: added if/else conditionals for folds when ak is multiple scalars
    if isinstance(ak, pd.Series) and len(ak)>1:
      ak = sign_CV(ak)
      num_term1 = sign(ak)
      num_term2 = abs(ak) - ((lambda_value)*(1-alpha_value))/2
      denominator = bk + (lambda_value)*(alpha_value)
      plus = max(0, num_term2)    
      B_k = (num_term1 * plus) / denominator
      return B_k
    
    else:
      num_term1 = sign(ak)
      num_term2 = abs(ak) - ((lambda_value)*(1-alpha_value))/2
      denominator = bk + (lambda_value)*(alpha_value)
      plus = max(0, num_term2)    
      B_k = (num_term1 * plus) / denominator
      return B_k # numpy.ndarray

####################################
# Coordinate Gradiant Descent Functions
####################################

def coordinate_descent_inner(X, Y, beta, alpha_value, lambda_value, bk):
  '''
  Function computes the coordinate descent and returns the updated parameter vector Bhat
  
  :param X: standardized Nxp design matrix of training data
  :param Y: centered N-dimensional response vector
  :param alpha_tuning: single alpha value out of the tuning vector list of 6 values
    (i.e. alpha_tuning = [0,1/5, 2/5, 3/5, 4/5, 1])
  :param lambda_tuning: single lambda value from the tuning vector, list of 9 values
    (lambda_tuning = [10**(-2), 10**(-1), 10**(0), 10, 10**(2), 10**(3), 10**(4), 10**(5), 10**(6)])
  :param max_iter: fixed iterations at 1000 (we assume max iters for convergence)
  :param b_k: precomputed b sub k vector across features

  :return beta: the beta parameter vector with updated values
  '''  
  p = X.shape[1]         # 9 columns
  beta = beta.T  
  # compute updated beta[k] value using a_k and b_k
  for k in range(p):                  # (9)
      ak = compute_a_k(beta,X,Y)
      beta[k] = compute_B_k(ak, lambda_value, alpha_value, bk[k])
  return beta

  def coordinate_descent(dX, dy, beta, alpha, lamb, bk, max_iter = 1000): 
  '''
  :param dX: X (Nxp design matrix)
  :param dy: Y centered data
  :parm alpha: alpha_tuning parameter vector
  :param lamb: lambda_tuning parameter vector
  :param bk: precomputed betak vector, use precompute_bk()
  :param max_iter: integer, total iterations allotted for convergence
  '''
  beta_old = beta
  beta_new = coordinate_descent_inner(dX, dy, beta_old, alpha, lamb, bk)
  for i in range(max_iter-1):
    if np.array_equal(beta_old, beta_new):
      return beta_new
    else:
      beta_old = beta_new
      beta_new = coordinate_descent_inner(dX, dy, beta_old, alpha, lamb, bk)
  return beta_new

####################################
# To obtain the coordinate descent use 
the following syntax to run the algorithm
using the functions above:
####################################
r = coordinate_descent(X, Y, beta_vector(), alpha_tuning[0], lambda_tuning[0], precompute_bk(), max_iter)
output1_iteration = pd.DataFrame(r).T
output1_iteration.columns = ['bhat0', 'bhat1', 'bhat2', 'bhat3', 'bhat4', 'bhat5', 'bhat6', 'bhat7', 'bhat8']
print('After 1 iteration check output:')
output1_iteration

####################################
# lambda function for graphing
####################################
def for_all_lambdas(dX, dy, beta, alpha, lamb, bk):
  # single alpha value
  # lamb is lambda tuning vector
  ''' function for graphs and plotting alphas'''
  param = pd.Series([])
  for i in range(len(lamb)):
    var = coordinate_descent(dX, dy, beta, alpha, lamb[i], bk)
    var = pd.Series(var, index = list(dX.columns))
    if param.empty:
      param = var 
    else:
      param = pd.concat([param,var], axis = 1)
    beta = beta_vector()
  param.columns = lamb
  param = param.T 
  return param
```

## All Source codes for computing Cross Validation with 5 folds

```
# Codes for Cross Validation

####################################
# Cross Validation Codes
####################################
df_val = clean.sample(frac=1).reset_index(drop=True)
# Used by passing the list of random numbers and removing them from the training indexes
# and from a list that keeps track of what already has been used

def rem_lst(org_ls,rem_ls,rand):
  for ele in rand:
    if ele in rem_ls:
      rem_ls.remove(ele)
    if ele in org_ls:
      org_ls.remove(ele)
  return org_ls, rem_ls, rand

####################################
# Standardize Validation
####################################
def val_stan(train, fold):
  ''' function that standardizes the validation sets '''
  a = train.subtract(train.mean())
  b = fold.subtract(train.mean())
  a1 = a.iloc[:,:-1]
  b1 = b.iloc[:,:-1]
  ty = a.iloc[:,-1]
  vy = b.iloc[:,-1]
  t = a1 / a1.std()
  v = b1 / a1.std()
  return t, ty, v, vy

  
```

```
####################################
# Folds and Training
####################################
import random
random.seed(9)
org_index = list(df_val.index)
r1,org1,k1 = rem_lst(org_index,org_index,random.sample(org_index,80))
# print(len(r1),len(org1),len(k1)) # For checking fold size

####################################
# Fold 1
####################################

fold1 = df_val.iloc[k1,:]
train1 = df_val.iloc[r1,:]
# y1 = df.iloc[k1,-1] # reponse vector
t1, ty1, f1, fy1 = val_stan(train1,fold1)
# new random beta vector for training CV
bk1 = t1.pow(2,axis = 1).sum(axis=0)

# alphas
a1_0 = for_all_lambdas(t1, ty1, beta_vector(), alpha_tuning[0], lambda_tuning, bk1) 
a1_1_5 = for_all_lambdas(t1, ty1, beta_vector(), alpha_tuning[1], lambda_tuning, bk1) 
a1_2_5 = for_all_lambdas(t1, ty1, beta_vector(), alpha_tuning[2], lambda_tuning, bk1) 
a1_3_5 = for_all_lambdas(t1, ty1, beta_vector(), alpha_tuning[3], lambda_tuning, bk1) 
a1_4_5 = for_all_lambdas(t1, ty1, beta_vector(), alpha_tuning[4], lambda_tuning, bk1) 
a1_1 = for_all_lambdas(t1, ty1, beta_vector(), alpha_tuning[5], lambda_tuning, bk1)

####################################
# Fold 2
####################################

random.seed(9)
org_index = list(df_val.index)
r2,org1,k2 = rem_lst(org_index,org1,random.sample(org1,80))
# print(len(r2),len(org1),len(k2))

fold2 = df_val.iloc[k2,:]
train2 = df_val.iloc[r2,:]
# y2 = df.iloc[k2,-1] 

t2, ty2, f2, fy2 = val_stan(train2, fold2)
bk2 = t2.pow(2,axis = 1).sum(axis=0)

#alphas
a2_0 = for_all_lambdas(t2, ty2, beta_vector(), alpha_tuning[0], lambda_tuning, bk2)
a2_1_5 = for_all_lambdas(t2, ty2, beta_vector(), alpha_tuning[1], lambda_tuning, bk2)
a2_2_5 = for_all_lambdas(t2, ty2, beta_vector(), alpha_tuning[2], lambda_tuning, bk2)
a2_3_5 = for_all_lambdas(t2, ty2, beta_vector(), alpha_tuning[3], lambda_tuning, bk2)
a2_4_5 = for_all_lambdas(t2, ty2, beta_vector(), alpha_tuning[4], lambda_tuning, bk2)
a2_1 = for_all_lambdas(t2, ty2, beta_vector(), alpha_tuning[5], lambda_tuning, bk2)

####################################
# Fold 3
####################################

random.seed(9)
org_index = list(df_val.index)
r3,org1,k3 = rem_lst(org_index,org1,random.sample(org1,80))
# print(len(r3),len(org1),len(k3))

fold3 = df_val.iloc[k3,:] 
train3 = df_val.iloc[r3,:] 
# y3 = df.iloc[k3,-1]

t3, ty3, f3, fy3 = val_stan(train3,fold3)
bk3 = t2.pow(2,axis = 1).sum(axis=0)

#alphas
a3_0 = for_all_lambdas(t3, ty3, beta_vector(), alpha_tuning[0], lambda_tuning, bk3)
a3_1_5 = for_all_lambdas(t3, ty3, beta_vector(), alpha_tuning[1], lambda_tuning, bk3)
a3_2_5 = for_all_lambdas(t3, ty3, beta_vector(), alpha_tuning[2], lambda_tuning, bk3)
a3_3_5 = for_all_lambdas(t3, ty3, beta_vector(), alpha_tuning[3], lambda_tuning, bk3)
a3_4_5 = for_all_lambdas(t3, ty3, beta_vector(), alpha_tuning[4], lambda_tuning, bk3)
a3_1 = for_all_lambdas(t3, ty3, beta_vector(), alpha_tuning[5], lambda_tuning, bk3)

####################################
# Fold 4
####################################

org_index = list(df_val.index)
r4,org1,k4 = rem_lst(org_index,org1,random.sample(org1,80))
print(len(r4),len(org1),len(k4))

fold4 = df_val.iloc[k4,:] 
train4 = df_val.iloc[r4,:] 
# y4 = df.iloc[k4,-1]

t4, ty4, f4, fy4 = val_stan(train4,fold4)
bk4 = t2.pow(2,axis = 1).sum(axis=0)

# alphas
a4_0 = for_all_lambdas(t4, ty4, beta_vector(), alpha_tuning[0], lambda_tuning, bk4)
a4_1_5 = for_all_lambdas(t4, ty4, beta_vector(), alpha_tuning[1], lambda_tuning, bk4)
a4_2_5 = for_all_lambdas(t4, ty4, beta_vector(), alpha_tuning[2], lambda_tuning, bk4)
a4_3_5 = for_all_lambdas(t4, ty4, beta_vector(), alpha_tuning[3], lambda_tuning, bk4)
a4_4_5 = for_all_lambdas(t4, ty4, beta_vector(), alpha_tuning[4], lambda_tuning, bk4)
a4_1 = for_all_lambdas(t4, ty4, beta_vector(), alpha_tuning[5], lambda_tuning, bk4)

####################################
# Fold 5
####################################

org_index = list(df_val.index)
r5,org1,k5 = rem_lst(org_index,org1,random.sample(org1,80))
print(len(r5),len(org1),len(k5))

fold5 = df_val.iloc[k5,:] 
train5 = df_val.iloc[r5,:] 
# y5 = df.iloc[k5,-1]

t5, ty5, f5, fy5 = val_stan(train5,fold5)
bk5 = t2.pow(2,axis = 1).sum(axis=0)

a5_0 = for_all_lambdas(t5, ty5, beta_vector(), alpha_tuning[0], lambda_tuning, bk5) 
a5_1_5 = for_all_lambdas(t5, ty5, beta_vector(), alpha_tuning[1], lambda_tuning, bk5) 
a5_2_5 = for_all_lambdas(t5, ty5, beta_vector(), alpha_tuning[2], lambda_tuning, bk5) 
a5_3_5 = for_all_lambdas(t5, ty5, beta_vector(), alpha_tuning[3], lambda_tuning, bk5) 
a5_4_5 = for_all_lambdas(t5, ty5, beta_vector(), alpha_tuning[4], lambda_tuning, bk5) 
a5_1 = for_all_lambdas(t5, ty5, beta_vector(), alpha_tuning[5], lambda_tuning, bk5)
```

## All Source codes to compute cost

```
####################################
# Cost Functions
####################################

def mse (X,y,beta,N):
  '''
  Calculates MSE
  :param X: standardized design matrix Nxp
  :param y: centered N-dimensional response vector
  :param beta: parameter vector
  :param N: integer representing row dimension of X; X.shape[0]
  '''
  mse_ls = list()
  for i in range(N):
    mse_ls.append((y.iloc[i]-(beta.dot(X.iloc[i])))**2)
  mse = sum(mse_ls)/N
  return mse

def mse_lamb(X,y,beta,elem):
  '''Calculates the mse for cross validation given a list of betas'''
  cv_ls = list()
  for i in range(elem):
    cv_ls.append(mse(X,y,beta.iloc[i],len(X)))
  return cv_ls

####################################
# Cost Calculations for CV Folds
####################################

fl1 = mse_lamb(f1,fy1,a1_0,len(a1_0)) 
fl2 = mse_lamb(f2,fy2,a2_0,len(a2_0)) 
fl3 = mse_lamb(f3,fy3,a3_0,len(a3_0)) 
fl4 = mse_lamb(f4,fy2,a4_0,len(a4_0)) 
fl5 = mse_lamb(f5,fy5,a5_0,len(a5_0))

fa0 = (mse_lamb(f1,fy1,a1_0,len(a1_0)),mse_lamb(f2,fy2,a2_0,len(a2_0)), 
      mse_lamb(f3,fy3,a3_0,len(a3_0)),mse_lamb(f3,fy3,a3_0,len(a3_0)), 
      mse_lamb(f4,fy2,a4_0,len(a4_0)),mse_lamb(f5,fy5,a5_0,len(a5_0)))

fa1 = (mse_lamb(f1,fy1,a1_1_5,len(a1_1_5)),mse_lamb(f2,fy2,a2_1_5,len(a2_1_5)),
       mse_lamb(f3,fy3,a3_1_5,len(a3_1_5)),mse_lamb(f3,fy3,a3_1_5,len(a3_1_5)), 
       mse_lamb(f4,fy2,a4_1_5,len(a4_1_5)),mse_lamb(f5,fy5,a5_1_5,len(a5_1_5)))

fa2 = (mse_lamb(f1,fy1,a1_2_5,len(a1_2_5)),mse_lamb(f2,fy2,a2_2_5,len(a2_2_5)), 
       mse_lamb(f3,fy3,a3_2_5,len(a3_2_5)),mse_lamb(f3,fy3,a3_2_5,len(a3_2_5)), 
       mse_lamb(f4,fy2,a4_2_5,len(a4_2_5)),mse_lamb(f5,fy5,a5_2_5,len(a5_2_5)))

fa3 = (mse_lamb(f1,fy1,a1_3_5,len(a1_3_5)),mse_lamb(f2,fy2,a2_3_5,len(a2_3_5)), 
       mse_lamb(f3,fy3,a3_3_5,len(a3_3_5)),mse_lamb(f3,fy3,a3_3_5,len(a3_3_5)), 
       mse_lamb(f4,fy2,a4_3_5,len(a4_3_5)),mse_lamb(f5,fy5,a5_3_5,len(a5_3_5)))

fa4 = (mse_lamb(f1,fy1,a1_4_5,len(a1_4_5)),mse_lamb(f2,fy2,a2_4_5,len(a2_4_5)), 
       mse_lamb(f3,fy3,a3_4_5,len(a3_4_5)),mse_lamb(f3,fy3,a3_4_5,len(a3_4_5)), 
       mse_lamb(f4,fy2,a4_4_5,len(a4_4_5)),mse_lamb(f5,fy5,a5_4_5,len(a5_4_5)))

fa5 = (mse_lamb(f1,fy1,a1_1,len(a1_1)),mse_lamb(f2,fy2,a2_1,len(a2_1)), 
       mse_lamb(f3,fy3,a3_1,len(a3_1)),mse_lamb(f3,fy3,a3_1,len(a3_1)), 
       mse_lamb(f4,fy2,a4_1,len(a4_1)),mse_lamb(f5,fy5,a5_1,len(a5_1)))
       
```

```
####################################
# Function for Output
####################################

def prep_cv_error(fold_list, index):
  cverr = pd.DataFrame()
  cverr['fold1'] = np.array(fold_list[0])
  cverr['fold2'] = np.array(fold_list[1])
  cverr['fold3'] = np.array(fold_list[2])
  cverr['fold4'] = np.array(fold_list[3])
  cverr['fold5'] = np.array(fold_list[4])
  cverr = cverr.rename(index={0:index[0],1:index[1],2:index[2],3:index[3],4:index[4],
                              5:index[5],6:index[6],7:index[7],8:index[8]})
  cverr_sum = cverr.sum(axis=1)/5
  return cverr, cverr_sum

####################################
# Output computation
####################################

cverr_df0,cverr0 = prep_cv_error(fa0,list(a1_0.index)) 
cverr_df1,cverr1 = prep_cv_error(fa1,list(a1_1_5.index)) 
cverr_df2,cverr2 = prep_cv_error(fa2,list(a1_2_5.index)) 
cverr_df3,cverr3 = prep_cv_error(fa3,list(a1_3_5.index)) 
cverr_df4,cverr4 = prep_cv_error(fa4,list(a1_4_5.index)) 
cverr_df5,cverr5 = prep_cv_error(fa5,list(a1_1.index))

####################################
# Convert to Pandas DataFrame
####################################

cverr = pd.DataFrame() 
cverr['0'] = np.array(cverr0) 
cverr['1/5'] = np.array(cverr1) 
cverr['2/5'] = np.array(cverr2) 
cverr['3/5'] = np.array(cverr3) 
cverr['4/5'] = np.array(cverr4) 
cverr['1'] = np.array(cverr5)
 # Labeling Columns
cverr = cverr.rename(index={0:cverr0.index[0],1:cverr0.index[1],2:cverr0. index[2],3:cverr0.index[3],
                              4:cverr0.index[4],5:cverr0.index[5],6:cverr0.index[6],7: cverr0.index[7],8:cverr0.index[8]})

# Printing output
cverr
```

## Source Code to generate CV Errors plot for all alphas tuning parameters

```
# Deliverable 2
sns.lineplot(data=cverr,dashes=False).set_title('CV Errors')
plt.xscale('log')
plt.xlabel("Lambda", fontsize=15)
plt.ylabel("CV Error", fontsize=15)
plt.show()
```

## Source Code to find smallest CV error with alpha and lambda values

```
# Deliverable 3
for i in cverr.index:
  for j in cverr.columns:
    if cverr.loc[i,j] == cverr.min().min(): 
      # print(i,j)
      row, col = i,j
print("Alpha of Min: ", col )
print("Lambda of Min: ", row)
print("Value of Min: ", cverr.loc[row,col])
```

# Deliverable 6

(extra credit): Implement the assignment using statistical or machine learning libraries in a language of your choice. Compare the results with those obtained above, and provide a discussion as to why you believe your results are different if you found them to be different. This is worth up to 10% additional credit, which would allow you to get up to 110% out of 100 for this assignment.
"""

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, mean_squared_error
from sklearn.linear_model import ElasticNet

D6 = ElasticNet(alpha = lambda_tuning[0], l1_ratio = alpha_tuning[0]).fit(X,Y)
D6 = D6.coef_
pd.DataFrame(D6).T

del1_extra = pd.DataFrame(index = pd.DataFrame(X).columns)

# Elastic Net Function
def elaNet(lamb_set, alpha, del1_extra, X, y):
  for i in lamb_set:
    en_extra = ElasticNet(alpha = i, l1_ratio = alpha)
    en_extra.fit(X,y)
    del1_extra[i] = en_extra.coef_
  return del1_extra

# Graphs and plotting
to_graph0 = elaNet(lambda_tuning, alpha_tuning[0], del1_extra, X, Y).T
to_graph1 = elaNet(lambda_tuning, alpha_tuning[1], del1_extra, X, Y).T
to_graph2 = elaNet(lambda_tuning, alpha_tuning[2], del1_extra, X, Y).T
to_graph3 = elaNet(lambda_tuning, alpha_tuning[3], del1_extra, X, Y).T
to_graph4 = elaNet(lambda_tuning, alpha_tuning[4], del1_extra, X, Y).T
to_graph5 = elaNet(lambda_tuning, alpha_tuning[5], del1_extra, X, Y).T

# Output
to_graph0

# Deliverable 1 Extra Credit

# alpha = 0
sns.lineplot(data=to_graph0,dashes=False).set_title('Alpha = 0')
plt.xscale('log')
plt.xlabel("Lambda", fontsize=15)
plt.ylabel("Standardized Coefficients", fontsize=15)
plt.show()
# alpha = 1/5 = 0.2
sns.lineplot(data=to_graph1,dashes=False).set_title('Alpha = 1/5')
plt.xscale('log')
plt.xlabel("Lambda", fontsize=15)
plt.ylabel("Standardized Coefficients", fontsize=15)
plt.show()
# alpha = 2/5 = 0.4
sns.lineplot(data=to_graph2,dashes=False).set_title('Alpha = 2/5') 
plt.xscale('log')
plt.xlabel("Lambda", fontsize=15)
plt.ylabel("Standardized Coefficients", fontsize=15)
plt.show()

# alpha = 3/5 = 0.6
sns.lineplot(data=to_graph3,dashes=False).set_title('Alpha = 3/5') 
plt.xscale('log')
plt.xlabel("Lambda", fontsize=15)
plt.ylabel("Standardized Coefficients", fontsize=15)
plt.show()
# alpha = 4/5 = 0.8
sns.lineplot(data=to_graph4,dashes=False).set_title('Aplha = 4/5') 
plt.xscale('log')
plt.xlabel("Lambda", fontsize=15)
plt.ylabel("Standardized Coefficients", fontsize=15)
plt.show()
# alpha = 1
sns.lineplot(data=to_graph5,dashes=False).set_title('Aplha = 1') 
plt.xscale('log')
plt.xlabel("Lambda", fontsize=15)
plt.ylabel("Standardized Coefficients", fontsize=15)
plt.show()

# Deliverable 2 Extra Credit
en = ElasticNet()
parameters = {'alpha': [10**-2, 10**-1, 10**0, 10**1, 10**2, 10**3, 10**4,10**5, 10**6],
              'l1_ratio': [0, 1/5, 2/5, 3/5, 4/5, 1]}
en_extra = GridSearchCV(en,parameters,scoring = make_scorer(mean_squared_error), cv=5)
en_extra.fit(X, Y)
# print(ridge_regression_extra.best_params_)
# print(ridge_regression_extra.best_score_)

"""

```
GridSearchCV(cv=5, error_score=nan,
             estimator=ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True,
                                  l1_ratio=0.5, max_iter=1000, normalize=False,
                                  positive=False, precompute=False,
                                  random_state=None, selection='cyclic',
                                  tol=0.0001, warm_start=False),
             iid='deprecated', n_jobs=None,
             param_grid={'alpha': [0.01, 0.1, 1, 10, 100, 1000, 10000, 100000,
                                   1000000],
                         'l1_ratio': [0, 0.2, 0.4, 0.6, 0.8, 1]},
             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,
             scoring=make_scorer(mean_squared_error), verbose=0)
```

"""

bee = en_extra.cv_results_['mean_test_score'] 
bee1 = pd.Series(bee[0:6], index= alpha_tuning)
bee2 = pd.Series(bee[6:12], index= alpha_tuning)
bee3 = pd.Series(bee[12:18], index= alpha_tuning) 
bee4 = pd.Series(bee[18:24], index= alpha_tuning) 
bee5 = pd.Series(bee[24:30], index= alpha_tuning) 
bee6 = pd.Series(bee[30:36], index= alpha_tuning) 
bee7 = pd.Series(bee[36:42], index= alpha_tuning)
bee8 = pd.Series(bee[42:48], index= alpha_tuning)
bee9 = pd.Series(bee[48:54], index= alpha_tuning)

p=pd.DataFrame(columns = lambda_tuning)
p[lambda_tuning[0]] = bee1
p[lambda_tuning[1]] = bee2
p[lambda_tuning[2]] = bee3
p[lambda_tuning[3]] = bee4
p[lambda_tuning[4]] = bee5
p[lambda_tuning[5]] = bee6
p[lambda_tuning[6]] = bee7
p[lambda_tuning[7]] = bee8
p[lambda_tuning[8]] = bee9
to_graph = p.T
to_graph

# Plot
sns.lineplot(data=to_graph,dashes=False) 
plt.xscale('log')
plt.xlabel("Lambda", fontsize=15) 
plt.ylabel("CV Error", fontsize=15) 
plt.show()

for i in to_graph.index:
  for j in to_graph.columns:
    if to_graph.loc[i,j] == bee.min():
      # print(i,j)
      row,col = i, j

bee.min()

# Deliverable 3 Extra credit
print("Alpha of Min: ", col )
print("Lambda of Min: ", row)
print("Value of Min: ", to_graph.loc[row,col])

# Deliverable 4 Extra credit
pd.DataFrame(to_graph4.loc[row,:]).T

"""# References

[1] [Lasso and Elastic Net Implementation](https://www.geeksforgeeks.org/implementation-of-lasso-ridge-and-elastic-net/)

[2] [Elastic Net Regression Tutorial](https://machinelearningmastery.com/elastic-net-regression-in-python/)

[3] [ML Example](https://medium.com/datadriveninvestor/a-practical-guide-to-getting-started-with-machine-learning-3a6fcc0f95aa)

[4] [Elastic Net Python Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)

[5] [Geeks for Geeks Example Implementation](https://www.geeksforgeeks.org/implementation-of-lasso-ridge-and-elastic-net/)

[6] [Train Split Model](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)

[7] [SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)

[8] [Coordinate Descent](https://xavierbourretsicotte.github.io/coordinate_descent.html)

[9] Peers who've helped me with this assignment: Whitney Andrews, Ernest Guagliata, Jean Perez Nieves
"""