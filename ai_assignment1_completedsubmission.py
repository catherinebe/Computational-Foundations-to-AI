# -*- coding: utf-8 -*-
"""AI Assignment1_CompletedSubmission
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/116lipi7fDGsYkHEE5aPE9AohCEuz0y0C
Catherine Berrouet, Computational Foundations of AI, Fall 2020, Assignment 1

# Task: Ridge Regression Fit
(From Sratch using Python)

You may **not** use a library that can perform *gradient descent, cross validation, ridge regression, least squares regression, optimization, etc.* to successfully complete this programing assignment. The goal of this assignment is not to learn how to use particular libraries of a language, but it is to instead understand how key methods in statistical machine learning are implemented. 
 

---

Opportunity for 10% extra credit if you additionally implement the assignment using built- in statistical or machine learning libraries (see Deliverable 6 at end of the document).

---

In this assignment you will be analyzing credit card data from ùëÅ = 400 training observations. The goal is to fit a model that can predict credit balance based on ùëù = 9 features describing an individual, which include an individual‚Äôs income, credit limit, credit rating, number of credit cards, age, education level, gender, student status, and marriage status. 

**Specifically, you will perform a penalized (regularized) least squares fit of a linear model using ridge regression, with the model parameters obtained by batch gradient descent. The tuning parameter will be chosen using five-fold cross validation, and the best-fit model parameters will be inferred on the training dataset conditional on an optimal tuning parameter.**

# Import Data and Libraries
"""

# Import Python libraries for data
import pandas as pd
import numpy as np
# Libraries for plotting
import matplotlib as mpl
import matplotlib.pyplot as plt
# import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

# Import data from csv file uploaded onto google drive
data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Computational AI/Credit_N400_p9.csv')
#data = pd.read_csv('/content/Credit_N400_p9.csv') #manual upload csv data file
data #prints data for viewing

"""## Cleaning and reformat raw data"""

# Reformatting categorical data into numerical binary values
datacopy = data # We use a copy and keep original import
clean = datacopy.replace({'Male': 0, 'Female':1})
clean = clean.replace({'No': 0, 'Yes': 1})
clean

"""## Initialize Variables for Training

### Algorithms
**Functions**
* total_predict: computes predictions
* residual: computes RSS
* cost: computes min total cost
* gradient: computes single gradient
* gradient descent: computes total gradients under max iterations

**Summary of Initialized Variables**

* Recall our parameters for use in the coded algorithms below: X, Y, N, p, beta
* X is our normalized training data centered and scaled to have unit standard deviation
* Y is the true value data generated as ùëÅ-dimensional centered response vector ùê≤
* N is the number of rows
* p is the number of columns should be = (dimension of beta)
* beta is our random initialized parameter vector

Now we will introduce some additional assumptions for our fitting as follows:
* tune is the Tuning Parameters Vector aka our lambda used in our Cost Function Equation
* iter is set to 1000
* alpha is initialized to 10^(-5) as suggested to act as proof of convergence within 1000 iterations
"""

# Training Data, X, Y, N, p
training_data = clean.iloc[:, :-1]                                # we only take the columns with 9 features we want to analyze
X = (training_data - training_data.mean())/training_data.std()    # X_normalized
Y_data = pd.DataFrame(clean.iloc[:, -1])                          # dependent variables; the true values y_i (i.e Credit Balance column)
Y_centered = Y_data - Y_data.mean(axis=0)                         # Y_centered
Y = pd.DataFrame(Y_centered)
N = X.shape[0]                                                    # 400 rows
p = X.shape[1]                                                    # 9 columns
# randomized initialization beta vector
random_initialization = np.random.uniform(low=-1.0, high=1.0, size=p)
beta = pd.DataFrame(np.random.uniform(low=-1.0, high=1.0, size=p))
# tuning parameter vector
tune = [10**(-2), 10**(-1), 10**(0), 10, 10**(2), 10**(3), 10**(4)]
max_iter = 1000
alpha = 10**(-5)

# Viewing all variables
print('Training Data')
print(training_data)
print('')
print('X')
print(X)
print('Y')
print(Y)
print('')
print('N =', N, ', p =', p)
print('')
print('beta = ', beta)
print('dimension:', beta.shape)
print('')
print('lambda tuning vector = ', tune)
print('')
print('alpha:', alpha)

"""# Training

## Prediction Function
"""

# Prediction Function
def total_predict(X, B):
  '''
    This function computes the dot product between the rows of design matrix and parameters column vector.
    It uses the predict function to iterate through each row of design matrix.
    :param X: training_data as pandas dataframe (i.e our Nxp design matrix)
    :param B: is parameters_vector as pandas dataframe syntax (i.e p-dimensional B)
    :return: predictions vector (i.e. dot product of X and beta parameters vector)
  '''
      # Corrective measure: Double check appropriate dimensions of dataframes
  dim1 = X.shape[1]
  dim2 = B.shape[0]
  if  (dim1) == (dim2):
    predictions = X.dot(B.to_numpy())
    return (predictions)
  else:
    print('Cannot compute. Please check Dimensions!')
    print('Dimensions of design matrix Nxp: ', X.shape)
    print('Dimensions of Initialized Parameter Vector: (', len(B), 'x 1)')

# Output
yhat = total_predict(X, beta)
print('Total predictions, yhat:')
print(yhat)
print('')
print('yhat Transposed First few entries:')
print((yhat.T).head(3))

"""## RSS Function"""

# RSS Function
def residual(Y, yhat):
  ''' 
  :param Y: true values
  :param yhat: predictions (i.e. dot product of X and parametric vector beta)
  :return: residual sum of sqaures
  '''
  dim1 = Y.shape
  dim2 = yhat.shape
  print("Y and yhat Dimensions Check:")
  print(dim1, 'and', dim2)
  rss = pd.DataFrame((Y.values - yhat.values)**2)
  return rss

# Output
residual_result = residual(Y,yhat)
print('rss:', residual_result)

"""## Cost Function"""

# Cost Function
def cost(rss, tuning, b):
  ''' 
  :param b: parametric_vector_B
  :param rss: residual sum of squares 
  :param tuning: tuning_parameter_vector
  :return: minimum cost computation
  '''
  # Checking pandas DataFrame dimensions
  tuning = pd.DataFrame(tuning)
  dim1, dim2, dim3 = rss.shape, tuning.shape, b.shape
  print('')
  print('rss Dimensions Check:', dim1)
  print('tuning parameter vector Dimensions Check:', dim2)
  print('b randomized vector Dimensions Check:', dim3)
  print('')
  # Cost Computation
  tobesummed = []
  for j in range(len(b)):
    compute = tuning @ ((b.iloc[j])**2)
    tobesummed.append(compute)
  regularization = sum(tobesummed)
  total_cost = rss.values + ((regularization).T).values
  # Convert to pandas Dataframe syntax
  return pd.DataFrame(total_cost)

# Output
rss1 = residual(Y, yhat)
cost_result = cost(rss1, tune, beta)
print('Cost Function Computation:')
print(cost_result)

"""## Single Gradient Function"""

# Single Gradient Computation Function
def gradient(X,Y, b, tuning_parameter_vector, alpha):
  ''' 
  This function computes the gradient for each b_j in the parametric vector B.
  :param X: training_data, this is our Nxp standardized matrix
  :param Y: normalized predictions, our yhats
  :param b: randomly initialized parametric vector, beta
  :param tuning_parameter_vector: our lambdas vector of 7 values
  :param alpha: starting point for learning

  Note: This is just 1 iteration to simply test gradient computation.
  '''
  # Comment: I've broken down each step of the computation mathematically 
  # in order to ensure the syntax for the pandas Dataframe is correct and precise
  for lambda_value in tuning_parameter_vector:
    for k in range(len(b)):
        step1 = Y.values -(X.dot(b.to_numpy()))
        X_t = X.iloc[:, :k] # X column k transposed
        step2 = (X_t).T  @ step1
        step3 = lambda_value * b.iloc[k] - step2
        step4 = 2 * lambda_value * step3
        step5 = b.iloc[k] - step4
        beta_update = step5
  return beta_update

# Output
print('Single gradient function computation (check):')
singlegradient = gradient(X,Y, beta, tune, alpha)
print(singlegradient)

"""## Gradient Descent Function"""

# Gradient Descent Function
def gd(X,Y,tune,alpha,max_iter):
  '''
  :param X: our standardized Nxp design matrix, <class 'pandas.core.frame.DataFrame>
  :param Y: our N-dimensional centered y
  :param beta: our randomply initialized parametric vector B
  :param max_iter: max iterations alloted for convergence
  :param alpha: proof of when convergence occurs (i.e. 2*alpha*lambda <1)
  :param L: lambda value in tuning parameter vector
  :return beta_update, cost: total gradient calculation and cost computation

  This function is used for 'training' phase/step.
  '''
  # Computations
  b = pd.DataFrame(data=np.random.uniform(-1, 1, X.shape[1]))
  XB = pd.DataFrame((X.values).dot(b.values))
  Y_minus_XB = pd.DataFrame(Y.values - XB.values)
  X_T_dotprod_Y_minus_XB = (X.T).dot(Y_minus_XB)

  # Change to dataframe syntax for computations
  tuning = pd.DataFrame(tune)
  Lb = tuning.dot(b.T)
  # Making pretty dataframe for lambabeta - for faster computations
  d = pd.Series(tune)
  lambda_beta_df = (Lb.T).rename(columns = d, inplace = False)
  # We will use this dataframe to plot a graph later

  # Iterate for every column in lambda_beta dataframe and subtract XB column
  for i in lambda_beta_df.columns:
    compute = lambda_beta_df[i].values - (X_T_dotprod_Y_minus_XB.iloc[:,0].values)

  # Iterate for 1000 iterations - this yields convergence by given assumption
  for iteration in range(max_iter):
    b_temp = (2*alpha)*(pd.DataFrame(compute))
    b_update = b - b_temp
  return b_update, lambda_beta_df # return new updated beta vector

# Output
gd_computation = (gd(X, Y, tune, alpha, 1000)[0])
to_plot = (gd(X, Y, tune, alpha, 1000)[1])
print('For gradient descent:')
print(gd_computation)

"""# Deliverable 1

Illustrate the effect of the tuning parameter on the inferred ridge regression coefficients by generating a plot of nine lines (one for each of the ùëù = 9 features), with the ùë¶-axis as $ùõΩ_ùëó = 1,2, ... ,9$ , and the ùë•-axis the corresponding log-scaled tuning parameter value $\log_{10}(ùúÜ)$ that generated the particular $\hat{ùõΩ}_j$. Label both axes. Without the log scaling of the tuning parameter, the plot will look distorted.
"""

d1 = pd.DataFrame(to_plot.T)
# Adding labels for corresponding b_j values in beta vector computed in dataframe
beta = pd.Series(['b1', 'b2', 'b3', 'b4', 'b5', 'b6', 'b7', 'b8', 'b9'])
d1 = (d1).rename(columns = beta, inplace = False)
# View our dataframe which we will use to plot for Deliverable 1
d1.T

# Notes for Plotting
# Each column for b_j's in the d1 dataframe will be the plotted y-axis points
# And the x-axis will consist of the lambda values on the log scale (first column of dataframe)

# Initialize figure
fig = plt.figure()
plt.title('Ridge Regression Fit')

# Plotting
for i in range(len(d1.columns)):
  # tuning parameter lambda and lambda_beta column values
  plt.plot(tune, d1.iloc[:, i])

# Axes
plt.xscale('log')
plt.xlabel('Tuning Parameter Values')
plt.ylabel('Features')

plt.show()
fig.savefig('Deliverable1.jpg')

"""# Task: Cross Validation

## Step 1: 

First we divide the data into K equal parts. Below I'm going to use the resulting dataframe with 80 rows and 5 columns.  Each column is a 'split' aka a fold. We will use the folds for the next step.

### Algorithms
**Functions**
* Cross Validation Split: computes folds
* CV: computes Cross Validation

**Summary of Initialized Variables**

* X_d2 is the normalized training data centered and scaled to have unit standard deviation
* Y_d2 is the true values data (i.e. Credit Balances) generated as ùëÅ-dimensional centered response vector ùê≤
* K is the number of folds

Same parameters as before repeated again as:

* b_d2 will be our random initialized parameter vector of length = dim k.
* tune is the Tuning Parameters Vector aka our lambda used in our Cost Function Equation
* maximum iteration is 1000
* alpha is initialized to 10^(-5) as suggested to act as proof of convergence within 1000 iterations

### Cross Validation Split Function
"""

# Set K parameter for K-fold Cross Validation
K = 5

# Import Library for randomization
import random
from random import randrange

# Cross Validation Split Function
def cross_validation_split(dataset, folds= K):
	dataset_split = list()
	dataset_copy = list(dataset)
	fold_size = int(len(dataset) / folds)
	for i in range(folds):
		fold = list()
		while len(fold) < fold_size:
			index = randrange(len(dataset_copy))
			fold.append(dataset_copy.pop(index))
		dataset_split.append(fold)
	return dataset_split

"""### Initialize Variables for Training

"""

# Grabbing 5 data folds from X data
random.seed(1)                                # random seed
                                              # d2 = (training_data.iloc[:,: -1]).values 
                                              # .values turnes into np array
d2 = X.values
X_folds = cross_validation_split(d2, K)       # cross validation
X_folds_df = (pd.DataFrame(X_folds))          # convert to dataframe
X_folds_df.T                                  # Transposed dataframe
                                              # Note: fold_size = total_rows / total_folds
groups = pd.Series(['k1', 'k2', 'k3', 'k4', 'k5'])
X_d2 = (X_folds_df.T).rename(columns = groups, inplace = False)
X_d2

# Grab the 5 datafolds for true value Y data
random.seed(1)                                   # random seed
# balance = (clean['Balance']).values          # np array
balance = Y.values
Y_folds = cross_validation_split(balance, K)   # cross validation
Y_folds_df = (pd.DataFrame(Y_folds))      # convert to dataframe
Y_folds_df.T                              # Transposed dataframe dim: 80 x 5 
# Note: there are 80 rows per fold as we expected: fold_size = total_rows / K
Y_d2 = (Y_folds_df.T).rename(columns = groups, inplace = False)
Y_d2

"""## Step 2:

Now that we've split the the data into k = 5 groups (columns in Step 1). 

So that we now run a 5-fold cross validation for every lambda in our tuning parameter vector and evaluated 5 times using the performance summarized by taking the mean performance score (using our gradient descent).

### Cross Validation Function
"""

# Cross Validation Algorithm
def CV(X_d2, Y_d2, tune, alpha):
  ''' 
  Cross Validation Function 
  :param X_d2: k-fold data dataframe
  :param Y_d2: true values fold data dataframe
  :param tune: tuning parameter of lambda values
  :param alpha: learning rate
  :return: beta updated dataframe for each fold, cost dataframe for each fold
  '''
  CV_5 = (gd(X_d2, Y_d2, tune, alpha, 1000))
  cv_beta = CV_5[0] # beta_update
  CV_5_beta = (cv_beta).rename(columns = groups, inplace = False)
  CV_5_cost = CV_5[1]
  CV_5_cost = (CV_5_cost.T).rename(columns = groups, inplace = False)
  return CV_5_beta, CV_5_cost

# Output for Viewing
# print('Cross Validation beta updates for each k-fold:')
# print(CV(X_d2, Y_d2, tune, alpha)[0])
# print('')
# print('Cost for each CV fold')
# print(CV(X_d2, Y_d2, tune, alpha)[1])

"""### **Cross Validation beta updates for each k-fold**"""

CV_5 = (gd(X_d2, Y_d2, tune, alpha, 1000)[0]) # beta_update
CV_5_beta = (CV_5.T).rename(columns = groups, inplace = False)
CV_5_beta

# for beta in CV_5_beta['k1'][0]:
#   print(beta)

"""### **Cost for each K-fold**"""

CV_5_cost = gd(X_d2, Y_d2, tune, alpha, 1000)[1].T # Cost
CV_5_cost= (CV_5_cost).rename(columns = groups, inplace = False)
CV_5_cost

"""# Deliverable 2

Illustrate the effect of the tuning parameter on the cross validation error by generating a plot  with the ùë¶-axis as $CV_{(5)}$ error, and the ùë•-axis the corresponding log-scaled tuning parameter value $\log_{10}(ùúÜ)$ that generated the particular $CV_{(5)}$ error. Label both axes. Without the log scaling of the tuning parameter, the $CV_{(5)}$ plot will look distorted.
"""

# Initialize figure
fig = plt.figure()
plt.title('5-Fold Cross Validation')

# Plotting
for i in range(len(CV_5_cost.columns)):
  # tuning parameter lambda and lambda_beta column values
  plt.plot(tune, CV_5_cost.iloc[:, i])

# Axes
plt.xscale('log')
plt.xlabel('Tuning Parameter Values')
plt.ylabel('CV5 error')

# Legend
# plt.legend(loc="upper left")

plt.show()
fig.savefig('Deliverable1.jpg')

"""# Deliverable 3

Indicate the value of ùúÜ that generated the smallest $CV_{(5)}$ error.
"""

# We need to find the minimum value of all the rows
# Then the row with the smallest set of values, 
# will correspond to the lambda which generated the smallest error.

# Get minimum errors list for every fold in Cost dataframe
for row,column in CV_5_cost.iterrows():
  min1 = min(CV_5_cost['k1']) # k1
  min2 = min(CV_5_cost['k2']) # k2
  min3 = min(CV_5_cost['k3']) # k3
  min4 = min(CV_5_cost['k4']) # k4
  min5 = min(CV_5_cost['k5']) # k5
  Total_Min = pd.Series([min1,min2,min3,min4,min5])

# Get minimum error of all folds minimums errors list
print('All Minimums for each fold')
print(Total_Min.values)
min_found =  min(Total_Min)
print('min:', min_found)

# Get lambda Value that corresponds to the minimum error
for i in range(len(Total_Min)):
  if Total_Min[i] == min_found:
    print('index:', i)

print('')
# CV_5_cost.iloc[0,i]
lowest_error_lambda = tune[i]
print('Lambda Value Found that generated the smallest CV(5) error was', lowest_error_lambda)
import math
print('That is Lambda 10^', math.log10(lowest_error_lambda))

"""# Deliverable 4

Given the optimal ùúÜ, retrain your model on the entire dataset of ùëÅ = 400 observations and provide the estimates of the ùëù = 9 best-fit model parameters.
"""

best_lambda = [10**2]
retrain = gd(X,Y,best_lambda,alpha,1000)
print('Retrained with the best lambda value, we get the following gradient descent:')

# Prince as Table with rows
col = pd.Series(['b1', 'b2','b3','b4','b5','b6','b7','b8', 'b9'])
d4 = pd.DataFrame(retrain[0])
# d4 = d4.T
# d4.rename(columns = col, inplace = False)

# Print as Column
as_col = pd.Series(['beta'])
d4.rename(columns = as_col, inplace = False)

"""# Deliverable 5

***Provide all your source code that you wrote from scratch to perform all analyses (aside from plotting scripts, which you do not need to turn in) in this assignment, along with instructions on how to compile and run your code.***

For the Ridge Regression Fitting, we have the following functions with their titles that compute the following:

1.   Prediction function: *total_predict*
2.   RSS Function: *residual*
3.   Cost Function: *cost*
4.   Gradient Descent: *gd*

Under each function are listed the description of each parameter necessary to run the function's algorithm. Underneath each function is also listed the "Output" that is a designated sample instructions/method as on how to use the function.

```
# Source Codes Listed

# Prediction Function
def total_predict(X, B):
  '''
    This function computes the dot product between the rows of design matrix and parameters column vector.
    It uses the predict function to iterate through each row of design matrix.
    :param X: training_data as pandas dataframe (i.e our Nxp design matrix)
    :param B: is parameters_vector as pandas dataframe syntax (i.e p-dimensional B)
    :return: predictions vector (i.e. dot product of X and beta parameters vector)
  '''
      # Corrective measure: Double check appropriate dimensions of dataframes
  dim1 = X.shape[1]
  dim2 = B.shape[0]
  if  (dim1) == (dim2):
    predictions = X.dot(B.to_numpy())
    return (predictions)
  else:
    print('Cannot compute. Please check Dimensions!')
    print('Dimensions of design matrix Nxp: ', X.shape)
    print('Dimensions of Initialized Parameter Vector: (', len(B), 'x 1)')

# Output
yhat = total_predict(X, beta)
print('Total predictions, yhat:')
print(yhat)
print('')
print('yhat Transposed First few entries:')
print((yhat.T).head(3))

---

# RSS Function
def residual(Y, yhat):
  ''' 
  :param Y: true values
  :param yhat: predictions (i.e. dot product of X and parametric vector beta)
  :return: residual sum of sqaures
  '''
  dim1 = Y.shape
  dim2 = yhat.shape
  print("Y and yhat Dimensions Check:")
  print(dim1, 'and', dim2)
  rss = pd.DataFrame((Y.values - yhat.values)**2)
  return rss

# Output
residual_result = residual(Y,yhat)
print('rss:', residual_result)

---

# Cost Function
def cost(rss, tuning, b):
  ''' 
  :param b: parametric_vector_B
  :param rss: residual sum of squares 
  :param tuning: tuning_parameter_vector
  :return: minimum cost computation
  '''
  # Checking pandas DataFrame dimensions
  tuning = pd.DataFrame(tuning)
  dim1, dim2, dim3 = rss.shape, tuning.shape, b.shape
  print('')
  print('rss Dimensions Check:', dim1)
  print('tuning parameter vector Dimensions Check:', dim2)
  print('b randomized vector Dimensions Check:', dim3)
  print('')
  # Cost Computation
  tobesummed = []
  for j in range(len(b)):
    compute = tuning @ ((b.iloc[j])**2)
    tobesummed.append(compute)
  regularization = sum(tobesummed)
  total_cost = rss.values + ((regularization).T).values
  # Convert to pandas Dataframe syntax
  return pd.DataFrame(total_cost)

# Output
rss1 = residual(Y, yhat)
cost_result = cost(rss1, tune, beta)
print('Cost Function Computation:')
print(cost_result)

---

# Gradient Descent Function
def gd(X,Y,tune,alpha,max_iter):
  '''
  :param X: our standardized Nxp design matrix, <class 'pandas.core.frame.DataFrame>
  :param Y: our N-dimensional centered y
  :param beta: our randomply initialized parametric vector B
  :param max_iter: max iterations alloted for convergence
  :param alpha: proof of when convergence occurs (i.e. 2*alpha*lambda <1)
  :param L: lambda value in tuning parameter vector
  :return beta_update, cost: total gradient calculation and cost computation

  This function is used for 'training' phase/step.
  '''
  # Computations
  b = pd.DataFrame(data=np.random.uniform(-1, 1, X.shape[1]))
  XB = pd.DataFrame((X.values).dot(b.values))
  Y_minus_XB = pd.DataFrame(Y.values - XB.values)
  X_T_dotprod_Y_minus_XB = (X.T).dot(Y_minus_XB)

  # Change to dataframe syntax for computations
  tuning = pd.DataFrame(tune)
  Lb = tuning.dot(b.T)
  # Making pretty dataframe for lambabeta - for faster computations
  d = pd.Series(tune)
  lambda_beta_df = (Lb.T).rename(columns = d, inplace = False)
  # We will use this dataframe to plot a graph later

  # Iterate for every column in lambda_beta dataframe and subtract XB column
  for i in lambda_beta_df.columns:
    compute = lambda_beta_df[i].values - (X_T_dotprod_Y_minus_XB.iloc[:,0].values)

  # Iterate for 1000 iterations - this yields convergence by given assumption
  for iteration in range(max_iter):
    b_temp = (2*alpha)*(pd.DataFrame(compute))
    b_update = b - b_temp
  return b_update, lambda_beta_df # return new updated beta vector

# Output
gd_computation = (gd(X, Y, tune, alpha, 1000)[0])
to_plot = (gd(X, Y, tune, alpha, 1000)[1])
print('For gradient descent:')
print(gd_computation)

---



```

For the Cross Validation, listed below are the algorithms source codes. Functions listed are as follows:



1.   Cross Validation Split Function: *cross_validation_split*
2.   Cross Validation Function: *CV*



```
# Set K parameter for K-fold Cross Validation
K = 5

# Import Library for randomization
import random
from random import randrange

# Cross Validation Split Function
def cross_validation_split(dataset, folds= K):
	dataset_split = list()
	dataset_copy = list(dataset)
	fold_size = int(len(dataset) / folds)
	for i in range(folds):
		fold = list()
		while len(fold) < fold_size:
			index = randrange(len(dataset_copy))
			fold.append(dataset_copy.pop(index))
		dataset_split.append(fold)
	return dataset_split

  ---
# Cross Validation Algorithm
def CV(X_d2, Y_d2, tune, alpha):
  ''' 
  Cross Validation Function 
  :param X_d2: k-fold data dataframe
  :param Y_d2: true values fold data dataframe
  :param tune: tuning parameter of lambda values
  :param alpha: learning rate
  :return: beta updated dataframe for each fold, cost dataframe for each fold
  '''
  CV_5 = (gd(X_d2, Y_d2, tune, alpha, 1000))
  cv_beta = CV_5[0] # beta_update
  CV_5_beta = (cv_beta).rename(columns = groups, inplace = False)
  CV_5_cost = CV_5[1]
  CV_5_cost = (CV_5_cost.T).rename(columns = groups, inplace = False)
  return CV_5_beta, CV_5_cost

# Output for Viewing
# print('Cross Validation beta updates for each k-fold:')
# print(CV(X_d2, Y_d2, tune, alpha)[0])
# print('')
# print('Cost for each CV fold')
# print(CV(X_d2, Y_d2, tune, alpha)[1])

# Recommended Visual Output for Viewing for beta updates output:

CV_5 = (gd(X_d2, Y_d2, tune, alpha, 1000)[0]) # beta_update
CV_5_beta = (CV_5.T).rename(columns = groups, inplace = False)
CV_5_beta

# Recommended Visual Output for Viewing for K-folds cost output:

CV_5_cost = gd(X_d2, Y_d2, tune, alpha, 1000)[1].T # Cost
CV_5_cost= (CV_5_cost).rename(columns = groups, inplace = False)
CV_5_cost

```
You will need to initialize the folds as follows to use the Cross Validation functions mentioned above:

```
# Grabbing 5 data folds from X data
random.seed(1)                                # random seed
                                              # d2 = (training_data.iloc[:,: -1]).values 
                                              # .values turnes into np array
d2 = X.values
X_folds = cross_validation_split(d2, K)       # cross validation
X_folds_df = (pd.DataFrame(X_folds))          # convert to dataframe
X_folds_df.T                                  # Transposed dataframe
                                              # Note: fold_size = total_rows / total_folds
groups = pd.Series(['k1', 'k2', 'k3', 'k4', 'k5'])
X_d2 = (X_folds_df.T).rename(columns = groups, inplace = False)
X_d2 

---

# Grab the 5 datafolds for true value Y data
random.seed(1)                                   # random seed
# balance = (clean['Balance']).values            # np array
balance = Y.values
Y_folds = cross_validation_split(balance, K)     # cross validation
Y_folds_df = (pd.DataFrame(Y_folds))      # convert to dataframe
Y_folds_df.T                              # Transposed dataframe dim: 80 x 5 
# Note: there are 80 rows per fold as we expected: fold_size = total_rows / K
Y_d2 = (Y_folds_df.T).rename(columns = groups, inplace = False)
Y_d2

---


```
"""
